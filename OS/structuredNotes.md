# Operating Systems - Quick Interview Prep Guide

## ğŸ“‹ Table of Contents
1. [Memory Management](#memory-management)
2. [Process & Thread Concepts](#process--thread-concepts)
3. [CPU Architecture](#cpu-architecture)
4. [Virtualization](#virtualization)
5. [System Monitoring](#system-monitoring)
6. [File System Concepts](#file-system-concepts)

---

## 1ï¸âƒ£ MEMORY MANAGEMENT

### ğŸ”¹ What is Paging?
**Quick Definition:** Memory management technique that divides memory into fixed-size blocks.

**Detailed Explanation:**
Paging divides both **virtual address space** (process memory) and **physical memory** (RAM) into fixed-size blocks:
- **Pages**: Virtual memory blocks (typically 4 KB)
- **Frames**: Physical memory blocks (same size as pages)

**How it works:**
```
Process Virtual Memory â†’ Page Table â†’ Physical RAM
    [Pages]          [Mapping]      [Frames]
```

**Key Components:**
- **Page Table**: Data structure maintained by OS that maps virtual pages to physical frames
- **Memory Management Unit (MMU)**: Hardware component that translates virtual addresses to physical addresses
- **Page Fault**: Occurs when a process tries to access a page not currently in RAM; OS must load it from disk

**Interview Points:**
âœ… Allows non-contiguous memory allocation (processes don't need continuous RAM blocks)
âœ… Enables efficient memory sharing between processes
âœ… Implements protection (each process has isolated address space)
âœ… Foundation for virtual memory implementation

---

### ğŸ”¹ What is Virtual Memory?
**Quick Definition:** Technique that gives each process the illusion of having large, continuous memory, even when physical RAM is limited.

**Detailed Explanation:**
Virtual memory combines **RAM** + **disk space (swap)** to create a larger addressable memory space.

**Key Concepts:**
- **Virtual Address Space**: The memory range a process "thinks" it has (e.g., 0 to 4 GB)
- **Swap Space**: Portion of disk used as overflow when RAM is full
- **Page Fault**: Interrupt triggered when accessing data not in RAM; OS loads it from swap

**Real-World Example:**
```
System has:        8 GB RAM + 8 GB Swap = 16 GB Virtual Memory
Process needs:     10 GB
Solution:          
  â†’ Active pages stay in RAM (fast access)
  â†’ Inactive pages move to swap (slower, but available)
  â†’ OS swaps pages in/out as needed
```

**Benefits:**
âœ… Run programs larger than physical RAM
âœ… Better multitasking (more processes can coexist)
âœ… Process isolation and security
âœ… Memory abstraction for developers

**Trade-off:** Swap is 100-1000Ã— slower than RAM, so excessive swapping ("thrashing") degrades performance.

---

### ğŸ”¹ Memory Hierarchy
**Quick Definition:** Organization of storage from fastest/smallest to slowest/largest.

**The Hierarchy:**
```
Speed â†‘ | Cost â†‘ | Size â†“

1. Registers          (CPU internal, nanoseconds, ~few KB)
2. L1 Cache           (per-core, ~nanoseconds, 32-64 KB)
3. L2 Cache           (per-core, ~ns, 256 KB-1 MB)
4. L3 Cache           (shared across cores, ~ns, 8-32 MB)
5. RAM                (main memory, ~100 ns, 8-64 GB)
6. SSD/HDD            (secondary storage, ms, 256 GB-TBs)
7. Network/Cloud      (tertiary, seconds, unlimited)

Speed â†“ | Cost â†“ | Size â†‘
```

**Key Principle:** 
**Locality of Reference**
- **Temporal Locality**: Recently accessed data will likely be accessed again soon
- **Spatial Locality**: Data near recently accessed addresses will likely be accessed next

**Interview Insight:** The hierarchy balances speed, cost, and capacity. Caching at each level reduces average access time dramatically.

---

### ğŸ”¹ What is Cache?
**Quick Definition:** Small, ultra-fast memory in CPU that stores frequently accessed data to reduce RAM access time.

**Cache Levels:**
- **L1 Cache**: Smallest (32-64 KB), fastest, per-core, split into instruction & data caches
- **L2 Cache**: Larger (256 KB-1 MB), per-core, unified cache
- **L3 Cache**: Largest (8-32 MB), shared across all cores, slowest of the three but still faster than RAM

**How Cache Works:**
1. CPU requests data
2. Check L1 â†’ if found (**cache hit**), use it
3. If not â†’ Check L2 â†’ if found, promote to L1
4. If not â†’ Check L3 â†’ if found, promote upward
5. If not (**cache miss**) â†’ Fetch from RAM (expensive!)

**Key Terms:**
- **Cache Hit**: Data found in cache (fast)
- **Cache Miss**: Data not in cache, must fetch from lower level (slow)
- **Cache Line**: Unit of data transfer (typically 64 bytes)
- **Cache Invalidation**: When data in cache becomes stale and must be refreshed

**Why It Matters:** RAM access is ~100Ã— slower than L1 cache. Good cache utilization = better performance.

---

## 2ï¸âƒ£ PROCESS & THREAD CONCEPTS

### ğŸ”¹ What is a Process?
**Quick Definition:** An independent program in execution with its own isolated memory and resources.

**Key Components:**
```
Process Structure:
â”œâ”€â”€ Code Segment        (program instructions)
â”œâ”€â”€ Data Segment        (global variables)
â”œâ”€â”€ Heap                (dynamic memory)
â”œâ”€â”€ Stack               (function calls, local variables)
â”œâ”€â”€ CPU Registers       (PC, SP, etc.)
â”œâ”€â”€ Page Table          (virtual memory mapping)
â””â”€â”€ File Descriptors    (open files, sockets)
```

**Characteristics:**
âœ… **Isolation**: Cannot directly access another process's memory
âœ… **Heavy**: Creating/switching processes is expensive
âœ… **Communication**: Via IPC (pipes, sockets, shared memory, message queues)
âœ… **Example**: Each Chrome tab is a separate process

**Interview Point:** Processes provide security and stability through isolationâ€”if one crashes, others survive.

---

### ğŸ”¹ What is a Thread?
**Quick Definition:** Smallest unit of execution within a process; shares memory with other threads in the same process.

**Shared vs. Private Resources:**
```
SHARED (among threads in same process):
â”œâ”€â”€ Code segment
â”œâ”€â”€ Data segment (global variables)
â”œâ”€â”€ Heap memory
â””â”€â”€ Open file descriptors

PRIVATE (per thread):
â”œâ”€â”€ Stack (local variables, function calls)
â”œâ”€â”€ Program Counter (current instruction)
â””â”€â”€ CPU Registers (thread state)
```

**Advantages:**
âœ… **Lightweight**: Faster to create and switch than processes
âœ… **Efficient**: Shared memory = no IPC overhead
âœ… **Parallelism**: Multiple threads can run on multiple cores simultaneously

**Example - Web Browser:**
```
Chrome Process
  â”œâ”€â”€ UI Thread          (renders interface)
  â”œâ”€â”€ Network Thread     (downloads resources)
  â”œâ”€â”€ JavaScript Thread  (executes scripts)
  â””â”€â”€ Rendering Thread   (paints pixels)
```
If JavaScript hangs, UI thread keeps responding.

---

### ğŸ”¹ What is Multithreading?
**Quick Definition:** Ability of a process to execute multiple threads concurrently, sharing the same memory space.

**Single-threaded vs. Multithreaded:**
```
Single-threaded:
Task A â†’ Task B â†’ Task C  (sequential)

Multithreaded:
Task A â†˜
Task B  â†’ All run concurrently
Task C â†—
```

**Benefits:**
âœ… **Responsiveness**: UI remains active while background tasks run
âœ… **Resource Sharing**: No need for expensive IPC
âœ… **Scalability**: Utilize multiple CPU cores
âœ… **Economy**: Cheaper than creating multiple processes

**Real-World Use Cases:**
- Web servers handling multiple requests
- Video games (rendering, physics, AI, audio)
- Database systems (query processing)

**Interview Insight:** Multithreading improves throughput and responsiveness but introduces complexity (race conditions, deadlocks).

---

### ğŸ”¹ Process States (Lifecycle)
**Quick Definition:** States a process transitions through during its lifetime.

**State Diagram:**
```
     New
      â†“
    Ready â†â€•â€•â€•â€•â€•â†’ Running â†â†’ Waiting
      â†‘             â†“
      â””â€•â€•â€•â€•â€•â€• Terminated
```

**State Descriptions:**
1. **New**: Process is being created
2. **Ready**: Waiting for CPU assignment (in ready queue)
3. **Running**: Executing on CPU
4. **Waiting/Blocked**: Waiting for I/O or event (e.g., disk read, user input)
5. **Terminated**: Finished execution, cleanup pending

**Transitions:**
- Ready â†’ Running: **Scheduler dispatches** process to CPU
- Running â†’ Ready: **Time slice expires** (preemption)
- Running â†’ Waiting: **I/O request** (voluntary)
- Waiting â†’ Ready: **I/O completes** (event occurs)

---

### ğŸ”¹ What is Context Switching? â­â­â­â­
**Quick Definition:** When CPU stops executing one process/thread and starts another by saving and restoring execution context.

**What Gets Saved/Restored:**
```
Context Includes:
â”œâ”€â”€ CPU Registers (Program Counter, Stack Pointer, etc.)
â”œâ”€â”€ Page Table Pointer (memory mapping)
â”œâ”€â”€ Process State (running, ready, etc.)
â””â”€â”€ File Descriptors (open files)
```

**Time Cost:** 1-10 microseconds per switch

**Why It's Expensive:**
Direct cost (small): Saving/loading registers  
**Indirect cost (BIG)**: **Cache Invalidation** ğŸ”¥

**Cache Invalidation Problem:**
```
Running Process A:
  CPU Cache filled with A's data (hot cache)
     â†“
Context Switch to Process B
     â†“
  CPU Cache now contains A's data (useless for B)
  B's data must be fetched from RAM (cold cache)
     â†“
  100Ã— slower access! This is the real penalty.
```

**Detection:**
Check context switches per second:
```bash
vmstat 1
# High "cs" column = excessive switching
```

**Interview Point:** Context switching overhead comes primarily from **cache misses**, not the register save/restore itself.

---

### ğŸ”¹ What is a Zombie Process?
**Quick Definition:** A process that has finished execution but still has an entry in the process table because its parent hasn't read its exit status.

**How It Happens:**
```
1. Child process finishes execution
2. Child calls exit() and enters zombie state
3. Parent should call wait() to read exit status
4. If parent doesn't call wait(), child remains zombie
```

**Characteristics:**
- Shows as `<defunct>` or `Z` in process list
- Takes up a PID slot (limited resource)
- No CPU or memory usage (just process table entry)
- Cannot be killed (already dead!)

**Solution:**
- Parent calls `wait()` or `waitpid()` to reap zombie
- If parent dies, init/systemd (PID 1) adopts and reaps it

**Interview Question:** "How do you handle zombies?"
**Answer:** Fix the parent process to properly call wait(), or restart the parent. Killing zombies doesn't workâ€”they're already terminated.

---

## 3ï¸âƒ£ CPU ARCHITECTURE

### ğŸ”¹ What are CPU Cores?
**Quick Definition:** Independent processing units within a CPU that can execute instructions simultaneously.

**Key Points:**
- **Single Core**: Executes one thread at a time (switches between tasks)
- **Multi-Core**: Each core can run a different thread truly in parallel
- **Example**: 8-core CPU can run 8 threads simultaneously

**Why More Cores Matter:**
âœ… Multitasking (running multiple applications)
âœ… Parallel workloads (video encoding, compilation, servers)
âœ… Better efficiency under heavy load

**Interview Insight:** More cores â‰  always faster. Single-threaded apps don't benefit from extra cores.

---

### ğŸ”¹ What is Clock Speed?
**Quick Definition:** Frequency at which a CPU core executes instructions, measured in GHz (billions of cycles per second).

**What It Means:**
- **3.5 GHz** = 3.5 billion cycles per second
- Higher clock speed = faster instruction execution **per core**
- Impacts single-threaded performance (gaming, application launch)

**Formula:**
```
Instructions Per Second = Clock Speed Ã— Instructions Per Cycle (IPC)
```

**Trade-off:**
- Higher clock speed = more heat and power consumption
- Modern CPUs boost clock speed dynamically (Turbo Boost)

---

### ğŸ”¹ Cores vs. Clock Speed
**Quick Summary:**
```
Clock Speed â†’ How FAST one core works (single-thread speed)
Core Count  â†’ How MANY tasks run AT ONCE (parallel throughput)
```

**Scenarios:**
- **Gaming, Office Apps**: High clock speed matters more (3.8+ GHz)
- **Video Editing, 3D Rendering, Servers**: More cores matter (8-16+)
- **Best CPU**: Balance of both (e.g., 8 cores @ 4.5 GHz)

**Interview Example:**
"For a database server handling 1000 concurrent queries, I'd choose a 16-core CPU even at lower clock speed, because parallelism matters more than single-thread speed."

---

### ğŸ”¹ What is Hyper-Threading?
**Quick Definition:** Intel's technology (Simultaneous Multi-Threading/SMT) that allows one physical core to execute two threads concurrently.

**How It Works:**
```
Physical Core 1:
  â”œâ”€â”€ Thread 1 (using ALU)
  â””â”€â”€ Thread 2 (waiting for memory)
      â†’ While Thread 2 waits, Thread 1 uses idle execution units
      â†’ Better core utilization!
```

**Key Points:**
- Duplicates: Registers, scheduling logic (small overhead)
- Shares: ALU, FPU, cache, execution units
- Result: ~20-30% performance gain (not 2Ã—)

**Example:**
4-core CPU with Hyper-Threading = 4 physical cores, 8 logical threads

**Interview Insight:** Hyper-Threading improves throughput on multi-threaded workloads but doesn't double performanceâ€”threads still share execution resources.

---

## 4ï¸âƒ£ VIRTUALIZATION

### ğŸ”¹ What is a Virtual Machine (VM)?
**Quick Definition:** Software-based emulation of a physical computer that runs its own OS and applications.

**Components:**
```
Physical Server (Host)
    â”œâ”€â”€ Hypervisor (KVM, VMware, Hyper-V)
        â”œâ”€â”€ VM 1 (Guest OS: Ubuntu)
        â”œâ”€â”€ VM 2 (Guest OS: Windows)
        â””â”€â”€ VM 3 (Guest OS: CentOS)
```

**Key Terms:**
- **Host OS**: Physical machine's operating system
- **Guest OS**: OS running inside VM
- **Hypervisor**: Software that creates and manages VMs; allocates resources

**Benefits:**
âœ… Resource consolidation (1 physical server â†’ 10 VMs)
âœ… Isolation (one VM crash doesn't affect others)
âœ… Easy snapshots and backups
âœ… Hardware abstraction (migrate VMs between servers)

---

### ğŸ”¹ How Does Paging Work in VMs?
**Quick Definition:** Virtualized environments use **two-level address translation** for memory management.

**Two-Level Paging:**
```
Guest OS Level:
  Guest Virtual Address â†’ Guest Physical Address
      (VM thinks this is real physical memory)
             â†“
Hypervisor Level:
  Guest Physical Address â†’ Host Physical Address
      (actual RAM on physical server)
```

**Process:**
1. Process in VM accesses virtual memory
2. VM's page table maps it to "guest physical" address
3. Hypervisor maps "guest physical" to actual host RAM

**Technology:** 
- **Nested Paging / EPT (Intel) / NPT (AMD)**: Hardware-assisted two-level translation
- Improves performance by reducing hypervisor intervention

**Interview Point:** "VMs add a layer of address translation, which is why nested paging hardware support is crucial for VM performance."

---

## 5ï¸âƒ£ SYSTEM MONITORING

### ğŸ”¹ What is Load Average? â­â­â­â­
**Quick Definition:** Average number of processes in **runnable** (ready/running) or **uninterruptible** (waiting for I/O) state over 1, 5, and 15 minutes.

**How to Read:**
```bash
$ uptime
15:23:45 up 10 days, load average: 2.15, 1.80, 1.50
                                    1min  5min  15min
```

**Interpretation (for 4-core system):**
```
Load < 4.0    â†’ Healthy, CPU has idle capacity
Load = 4.0    â†’ Fully utilized, no waiting
Load > 4.0    â†’ Processes waiting for CPU (overloaded)
```

**Trend Analysis:**
- **Increasing** (1.5 â†’ 1.8 â†’ 2.1): Problem getting worse
- **Decreasing** (2.5 â†’ 2.0 â†’ 1.5): System recovering
- **Stable**: Predictable load

**Common Mistake:** Load average includes I/O wait, not just CPU! High load doesn't always mean CPU bottleneckâ€”could be disk I/O.

**Interview Question:** "Load average is 8.0 on a 4-core system. What do you check?"
**Answer:** 
1. `top` - Check CPU% vs. I/O wait%
2. `iostat` - Check if disk is bottleneck
3. `ps aux --sort=-%cpu` - Identify CPU-hungry processes

---

### ğŸ”¹ What is OOM Killer?
**Quick Definition:** Linux kernel mechanism that kills processes when system runs out of memory (RAM + swap exhausted).

**How It Works:**
```
1. System runs out of memory
2. Kernel calculates OOM score for each process
3. Kills process with highest score
4. Frees memory for other processes
```

**OOM Score Factors:**
- Memory usage (higher = more likely to be killed)
- Process importance (system processes protected)
- Runtime (newer processes more likely killed)
- Nice value (lower priority = higher score)

**Check OOM Scores:**
```bash
cat /proc/*/oom_score | sort -n
```

**Interview Question:** "How do you prevent OOM kills?"
**Answer:**
1. Add more RAM or swap
2. Set `oom_score_adj` to protect critical processes
3. Configure memory limits (cgroups, ulimit)
4. Monitor memory usage proactively

---

## 6ï¸âƒ£ FILE SYSTEM CONCEPTS

### ğŸ”¹ What is an Inode? â­â­â­
**Quick Definition:** Data structure that stores file metadata; each file has exactly one inode.

**What Inode Stores:**
```
Inode Contains:
â”œâ”€â”€ File permissions (rwxr-xr-x)
â”œâ”€â”€ Owner (UID) and Group (GID)
â”œâ”€â”€ File size (in bytes)
â”œâ”€â”€ Timestamps (created, modified, accessed)
â”œâ”€â”€ Link count (number of hard links)
â”œâ”€â”€ Pointers to data blocks (where actual data is stored)
â””â”€â”€ File type (regular, directory, symlink, etc.)
```

**What Inode DOES NOT Store:**
- âŒ Filename (stored in directory entry)
- âŒ Actual file data (stored in data blocks)

**How File Access Works:**
```
1. User opens "file.txt"
2. OS looks up filename in directory â†’ finds inode number
3. OS reads inode â†’ gets metadata and data block pointers
4. OS reads data blocks â†’ returns file content
```

**Interview Question:** "Can multiple filenames point to the same inode?"
**Answer:** Yes! **Hard links** allow multiple directory entries to reference the same inode. Example:
```bash
ln file.txt hardlink.txt  # Both point to same inode
```

**Running Out of Inodes:**
Even with free disk space, you can't create new files if inodes are exhausted (common with many small files).

**Check Inode Usage:**
```bash
df -i   # Shows inode usage per filesystem
```

---

## ğŸ¯ INTERVIEW STRATEGY TIPS

### Question Pattern Recognition:
1. **Definition Questions**: "What is X?" â†’ Give 2-sentence definition + key components
2. **Scenario Questions**: "System is slow, what do you check?" â†’ Systematic troubleshooting steps
3. **Comparison Questions**: "Process vs. Thread?" â†’ Create comparison table
4. **Deep-Dive Questions**: "Explain how context switching works" â†’ Step-by-step with diagrams

### Common Follow-ups to Expect:
- **After Paging**: "What happens during a page fault?"
- **After Process/Thread**: "How do they communicate?"
- **After Load Average**: "Load is 10 on 4-core system, what do you do?"
- **After Cache**: "What is cache coherence in multi-core systems?"
- **After Context Switching**: "How do you reduce context switches?"

### Golden Rule:
**Start simple, go deep if asked.** Don't over-explain unless the interviewer probes deeper.

---

## ğŸ“Š QUICK COMPARISON TABLES

### Process vs. Thread
| Aspect | Process | Thread |
|--------|---------|--------|
| **Memory** | Separate address space | Shared address space |
| **Creation** | Expensive (ms) | Cheap (Î¼s) |
| **Communication** | IPC (pipes, sockets) | Shared memory (direct) |
| **Crash Impact** | Isolated | Affects all threads |
| **Use Case** | Isolation needed | Parallelism within app |

### Cache Levels
| Level | Size | Speed | Scope |
|-------|------|-------|-------|
| **L1** | 32-64 KB | ~1 ns | Per-core |
| **L2** | 256 KB-1 MB | ~4 ns | Per-core |
| **L3** | 8-32 MB | ~12 ns | Shared |
| **RAM** | 8-64 GB | ~100 ns | System |

---

**Last Updated:** November 2025  
**Focus Areas:** Memory management, process scheduling, CPU architecture, virtualization basics
